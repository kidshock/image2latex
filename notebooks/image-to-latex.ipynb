{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torchdata torch jiwer torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git init .\n",
    "# !git remote add origin https://github.com/tuanio/image2latex.git\n",
    "# !git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/img2latex_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, Tensor\n",
    "# import torchdata.datapipes as dp\n",
    "import matplotlib.pyplot as plt\n",
    "from image2latex import Image2Latex, Text\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from jiwer import wer as cal_wer\n",
    "from nltk.metrics import edit_distance\n",
    "from typing import Tuple\n",
    "import pytorch_lightning as pl\n",
    "from tri_stage_lr_scheduler import TriStageLRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = Path('im2latex100k')\n",
    "img_path = Path('im2latex100k/formula_images_processed/formula_images_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "# mini_bs = 2\n",
    "lr = 1e-3\n",
    "epochs = 15\n",
    "max_length = 150\n",
    "log_idx = 300\n",
    "workers = 12\n",
    "\n",
    "cuda = torch.cuda.is_available()  \n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text()\n",
    "n_class = len(text.tokens)\n",
    "n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatexDataset(Dataset):\n",
    "    def __init__(self, data_type: str):\n",
    "        super().__init__()\n",
    "        assert data_type in ['train', 'test', 'validate'], 'Not found data type'\n",
    "        csv_path = data_path / f'im2latex_{data_type}.csv'\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['image'] = df.image.map(lambda x: img_path / x)\n",
    "        self.walker = df.to_dict('records')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.walker)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.walker[idx]\n",
    "        \n",
    "        formula = item['formula']\n",
    "        image = torchvision.io.read_image(str(item['image']))\n",
    "        \n",
    "        return image, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75275, 8370, 10355)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = LatexDataset('train')\n",
    "val_set = LatexDataset('validate')\n",
    "test_set = LatexDataset('test')\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18819"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = round(len(train_set) / bs)\n",
    "\n",
    "warmup_epochs = 2\n",
    "constant_epochs = 8\n",
    "decay_epochs = 5\n",
    "\n",
    "assert warmup_epochs + constant_epochs + decay_epochs == epochs, \"Not equal\"\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    formulas = [text.text2int(i[1]) for i in batch]\n",
    "    formulas = pad_sequence(formulas, batch_first=True)\n",
    "    sos = torch.zeros(bs, 1) + text.map_tokens['<s>']\n",
    "    eos = torch.zeros(bs, 1) + text.map_tokens['<e>']\n",
    "    formulas = torch.cat((sos, formulas, eos), dim=-1).to(dtype=torch.long)\n",
    "    image = [i[0] for i in batch]\n",
    "    max_width, max_height = 0, 0\n",
    "    for img in image:\n",
    "        c, h, w = img.size()\n",
    "        max_width = max(max_width, w)\n",
    "        max_height = max(max_height, h)\n",
    "    pad = torchvision.transforms.Resize(size=(max_height, max_width))\n",
    "    image = torch.stack(list(map(lambda x: pad(x), image))).to(dtype=torch.float)\n",
    "    return image, formulas\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_set, val_set, test_set):\n",
    "        super().__init__()\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        self.test_set = test_set\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set,\n",
    "                          shuffle=True, \n",
    "                          batch_size=bs,\n",
    "                          num_workers=workers,\n",
    "                          collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set,\n",
    "                          shuffle=False, \n",
    "                          batch_size=bs,\n",
    "                          num_workers=workers,\n",
    "                          collate_fn=collate_fn)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set,\n",
    "                          shuffle=False, \n",
    "                          batch_size=bs,\n",
    "                          num_workers=workers,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2LatexModel(pl.LightningModule):\n",
    "    def __init__(self, lr=lr, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = Image2Latex(**kwargs)\n",
    "        self.criterion = nn.CrossEntropyLoss() \n",
    "        self.lr = lr\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = TriStageLRScheduler(optimizer,\n",
    "                                        init_lr=1e-4,\n",
    "                                        peak_lr=1e-3,\n",
    "                                        final_lr=1e-5,\n",
    "                                        init_lr_scale=0.01,\n",
    "                                        final_lr_scale=0.01,\n",
    "                                        warmup_steps=steps_per_epoch * warmup_epochs,\n",
    "                                        hold_steps=steps_per_epoch * constant_epochs,\n",
    "                                        decay_steps=steps_per_epoch * decay_epochs,\n",
    "                                        total_steps=steps_per_epoch * bs)\n",
    "        scheduler = {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step', # or 'epoch'\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def forward(self, images, formulas):\n",
    "        return self.model(images, formulas)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, formulas = batch\n",
    "        \n",
    "        formulas_in = formulas[:, :-1]\n",
    "        formulas_out = formulas[:, 1:]\n",
    "        \n",
    "        outputs = self.model(images, formulas_in)\n",
    "        \n",
    "        bs, t, _ = outputs.size()\n",
    "        _o = outputs.reshape(bs * t, -1)\n",
    "        _t = formulas_out.reshape(-1)\n",
    "        loss = self.criterion(_o, _t)\n",
    "        \n",
    "        self.log(\"train loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, formulas = batch\n",
    "        \n",
    "        formulas_in = formulas[:, :-1]\n",
    "        formulas_out = formulas[:, 1:]\n",
    "\n",
    "        outputs = self.model(images, formulas_in)\n",
    "\n",
    "        bs, t, _ = outputs.size()\n",
    "        _o = outputs.reshape(bs * t, -1)\n",
    "        _t = formulas_out.reshape(-1)\n",
    "        loss = self.criterion(_o, _t)\n",
    "        perplexity = torch.exp(loss)\n",
    "\n",
    "        predicts = [text.tokenize(self.model.decode(i.unsqueeze(0), max_length)) for i in images]\n",
    "        truths = [text.tokenize(text.int2text(i)) for i in formulas]\n",
    "\n",
    "        edit_dist = torch.mean(torch.Tensor([edit_distance(pre, tru) / max(len(pre), len(tru)) for pre, tru in zip(predicts, truths)]))\n",
    "        \n",
    "        self.log('val loss', loss)\n",
    "        self.log('val perplexity', perplexity)\n",
    "        self.log('val edit distance', edit_dist)\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, formulas = batch\n",
    "        \n",
    "        formulas_in = formulas[:, :-1]\n",
    "        formulas_out = formulas[:, 1:]\n",
    "\n",
    "        outputs = self.model(images, formulas_in)\n",
    "\n",
    "        bs, t, _ = outputs.size()\n",
    "        _o = outputs.reshape(bs * t, -1)\n",
    "        _t = formulas_out.reshape(-1)\n",
    "        loss = self.criterion(_o, _t)\n",
    "        perplexity = torch.exp(loss)\n",
    "\n",
    "        predicts = [text.tokenize(self.model.decode(i.unsqueeze(0), max_length)) for i in images]\n",
    "        truths = [text.tokenize(text.int2text(i)) for i in formulas]\n",
    "\n",
    "        edit_dist = torch.mean(torch.Tensor([edit_distance(pre, tru) / max(len(pre), len(tru)) for pre, tru in zip(predicts, truths)]))\n",
    "        \n",
    "        self.log('test loss', loss)\n",
    "        self.log('test perplexity', perplexity)\n",
    "        self.log('test edit distance', edit_dist)\n",
    "    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/img2latex_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 80\n",
    "dec_dim = 256\n",
    "enc_dim = 256\n",
    "attn_dim = 256\n",
    "\n",
    "model = Image2LatexModel(lr=lr, n_class=n_class, emb_dim=emb_dim, enc_dim=enc_dim,\n",
    "                         dec_dim=dec_dim, attn_dim=attn_dim, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Image2Latex      | 4.5 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "4.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.5 M     Total params\n",
      "17.995    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|██▋                                                                                                                                                     | 363/20912 [01:09<1:05:16,  5.25it/s, loss=6.25, v_num=1]"
     ]
    }
   ],
   "source": [
    "tb_logger = pl.loggers.tensorboard.TensorBoardLogger('tb_logs', name='image2latex_model')\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(logger=tb_logger, callbacks=[lr_monitor],\n",
    "                     max_epochs=epochs, accelerator='gpu', accumulate_grad_batches=16)\n",
    "trainer.fit(datamodule=dm, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1906cf1cc7fa6408e13ff21f4559831c362da04cbd83a034b1d13ae422e8060c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
